{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0a1efc",
   "metadata": {},
   "source": [
    "# **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d01947da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a6bc1",
   "metadata": {},
   "source": [
    "# **Part 4: Model Distillation/Quantization**\n",
    "\n",
    "### **a. Model Distillation/Quantization**: \n",
    "Distill/Quantize your best-performing model into a lighter model. Document the process and tools used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e60c0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "path_model = os.path.join(path, 'saved_teacher_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa89e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your fine-tuned teacher model\n",
    "teacher_model = BertForSequenceClassification.from_pretrained(path_model).eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0209a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Financial Phrasebank Dataset (assuming you created a custom version)\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_75agree\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Preprocessing\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized = train_dataset.map(tokenize, batched=True)\n",
    "tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39e3f015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "student_model.train()\n",
    "\n",
    "train_loader = DataLoader(tokenized, batch_size=16)\n",
    "\n",
    "# Define loss and optimizer\n",
    "kd_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(student_model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(input_ids, attention_mask=attention_mask).logits\n",
    "            soft_labels = torch.nn.functional.softmax(teacher_logits / 2.0, dim=-1)\n",
    "\n",
    "        student_logits = student_model(input_ids, attention_mask=attention_mask).logits\n",
    "        student_log_probs = torch.nn.functional.log_softmax(student_logits / 2.0, dim=-1)\n",
    "\n",
    "        kd_loss = kd_loss_fn(student_log_probs, soft_labels)\n",
    "        ce_loss = ce_loss_fn(student_logits, labels)\n",
    "        loss = 0.5 * kd_loss + 0.5 * ce_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "student_model.save_pretrained(\"distilled_student_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9d75d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    student_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "torch.save(quantized_model.state_dict(), \"quantized_student_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31270ce5",
   "metadata": {},
   "source": [
    "### **b. Performance and Speed Comparison**: \n",
    "Evaluate the distilled model's performance and inference speed compared to the original. Highlight key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d4e6834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Inference Time Benchmarking:\n",
      "Original (Teacher) Inference Time: 0.1365s\n",
      "Distilled (Student) Inference Time: 0.0516s\n",
      "Quantized (Student) Inference Time: 0.0450s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sobottka/anaconda3/envs/textmining/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Original (Teacher) Evaluation Metrics:\n",
      "Accuracy:  0.6215\n",
      "Precision: 0.3862\n",
      "Recall:    0.6215\n",
      "F1-score:  0.4764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sobottka/anaconda3/envs/textmining/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Distilled (Student) Evaluation Metrics:\n",
      "Accuracy:  0.1216\n",
      "Precision: 0.0148\n",
      "Recall:    0.1216\n",
      "F1-score:  0.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sobottka/anaconda3/envs/textmining/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Quantized (Student) Evaluation Metrics:\n",
      "Accuracy:  0.1216\n",
      "Precision: 0.0148\n",
      "Recall:    0.1216\n",
      "F1-score:  0.0264\n",
      "\n",
      "üîç Custom Sentence Predictions:\n",
      "\n",
      "üìù Sentence: The company's performance was exceptional this quarter\n",
      "  üîπ Teacher Prediction: 1 | ‚ùå\n",
      "  üîπ Student Prediction: 0 | ‚ùå\n",
      "  üîπ Quantized Prediction: 0 | ‚ùå\n",
      "\n",
      "üìù Sentence: The results were below expectations and disappointed investors\n",
      "  üîπ Teacher Prediction: 1 | ‚ùå\n",
      "  üîπ Student Prediction: 0 | ‚úÖ\n",
      "  üîπ Quantized Prediction: 0 | ‚úÖ\n",
      "\n",
      "üìù Sentence: The company held a press conference regarding its quarterly report\n",
      "  üîπ Teacher Prediction: 1 | ‚úÖ\n",
      "  üîπ Student Prediction: 0 | ‚ùå\n",
      "  üîπ Quantized Prediction: 0 | ‚ùå\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Reproducibility ---\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# --- Load Models ---\n",
    "teacher_model = BertForSequenceClassification.from_pretrained(\"saved_teacher_model\").to(device)\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained(\"distilled_student_model\").to(device)\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    student_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ").to(device)\n",
    "\n",
    "# --- Inference Time Benchmarking ---\n",
    "def benchmark(model, inputs, label):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        _ = model(**inputs)\n",
    "        elapsed = time.time() - start\n",
    "    print(f\"{label} Inference Time: {elapsed:.4f}s\")\n",
    "\n",
    "# Single sentence for timing\n",
    "timing_input = tokenizer(\"The company's performance was exceptional this quarter\", return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "timing_input.pop(\"token_type_ids\", None)\n",
    "timing_input = {k: v.to(device) for k, v in timing_input.items()}\n",
    "\n",
    "print(\"‚è±Ô∏è Inference Time Benchmarking:\")\n",
    "benchmark(teacher_model, timing_input, \"Original (Teacher)\")\n",
    "benchmark(student_model, timing_input, \"Distilled (Student)\")\n",
    "benchmark(quantized_model, timing_input, \"Quantized (Student)\")\n",
    "\n",
    "# --- Evaluation on Full Dataset ---\n",
    "def evaluate_model(model, dataloader, label):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\nüìä {label} Evaluation Metrics:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "# Create DataLoader from your tokenized dataset\n",
    "test_loader = DataLoader(tokenized, batch_size=32)\n",
    "\n",
    "evaluate_model(teacher_model, test_loader, \"Original (Teacher)\")\n",
    "evaluate_model(student_model, test_loader, \"Distilled (Student)\")\n",
    "evaluate_model(quantized_model, test_loader, \"Quantized (Student)\")\n",
    "\n",
    "# --- Custom Example Sentences (Manual Evaluation) ---\n",
    "examples = [\n",
    "    (\"The company's performance was exceptional this quarter\", 2),  # Positive\n",
    "    (\"The results were below expectations and disappointed investors\", 0),  # Negative\n",
    "    (\"The company held a press conference regarding its quarterly report\", 1),  # Neutral\n",
    "]\n",
    "\n",
    "def predict(model, sentence):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs.pop(\"token_type_ids\", None)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        return torch.argmax(probs, dim=1).item()\n",
    "\n",
    "print(\"\\nüîç Custom Sentence Predictions:\\n\")\n",
    "for text, true_label in examples:\n",
    "    print(f\"üìù Sentence: {text}\")\n",
    "    for name, model in [(\"Teacher\", teacher_model), (\"Student\", student_model), (\"Quantized\", quantized_model)]:\n",
    "        pred = predict(model, text)\n",
    "        correct = \"‚úÖ\" if pred == true_label else \"‚ùå\"\n",
    "        print(f\"  üîπ {name} Prediction: {pred} | {correct}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eee0c5",
   "metadata": {},
   "source": [
    "### **c. Analysis and Improvements**: \n",
    "Analyze deficiencies in the student model's learning. Suggest potential improvements or further research directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2f1d9",
   "metadata": {},
   "source": [
    "The student and especially the quantized model show significant performance degradation when compared to the original teacher model:\n",
    "** Observed Deficiencies:**\n",
    "\n",
    "- **Very Low Accuracy and F1-Score:**\n",
    "    - The quantized student model has an accuracy of only ~12%, with F1-score near zero, indicating it is nearly guessing or predicting one class consistently.\n",
    "    - Even the unquantized student model performs poorly on custom examples, suggesting distillation failed to retain key patterns from the teacher.\n",
    "\n",
    "- **Mode Collapse / Class Bias:**\n",
    "    - The student and quantized models overpredict class 0 (Negative), regardless of actual sentiment.\n",
    "    - This indicates poor generalization and possible class imbalance or overfitting to frequent negative samples.\n",
    "\n",
    "- **Teacher Errors:**\n",
    "    - Even the teacher incorrectly predicted class 1 (Neutral) for a clearly Positive sentence, suggesting potential label noise or inadequate fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131c6f1",
   "metadata": {},
   "source": [
    "**Suggested Improvements & Research Directions:**\n",
    "\n",
    "- **Better Distillation Process:**\n",
    "    - Use soft-labels (logits/softmax outputs) from the teacher instead of just hard labels.\n",
    "    - Apply temperature scaling during distillation to preserve class probability distributions.\n",
    "\n",
    "- **Data Augmentation:**\n",
    "    - The Financial Phrasebank is relatively small. Apply paraphrasing, back-translation, or synonym replacement to create richer training samples for the student.\n",
    "\n",
    "- **Balance the Dataset:**\n",
    "    - Analyze the class distribution. If imbalanced, apply class-weighted loss or oversampling for minority classes.\n",
    "\n",
    "- **Quantization-Aware Training (QAT):**\n",
    "    - Instead of post-training quantization, train the student model with quantization simulated during training to preserve accuracy.\n",
    "\n",
    "- **Error Analysis & Curriculum Learning:**\n",
    "    - Identify hard-to-classify examples and apply focused retraining or curriculum learning to guide the student through easier to harder samples.\n",
    "\n",
    "- **Layer-Wise Distillation:**\n",
    "    - Instead of only distilling final logits, distill hidden representations (intermediate features) to better capture the teacher‚Äôs knowledge. Improvements & Research Directions:\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
